{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef733f21",
   "metadata": {},
   "source": [
    "# Function and Class Testing\n",
    "\n",
    "Notebook for testing functions and classes prior to implementation in the assignment notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1acbb395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281b88b9",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Modify function load data in the Corpus class to read in the text data and fill in\n",
    "the id to word, word to id, and full token sequence as ids fields. You can safely\n",
    "skip the rare word removal and subsampling for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "nonprofit-relaxation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data and tokenizing\n",
      "---------------------------------------\n",
      "['career', 'from', 'june', '2009', 'to', 'february', '2013', 'sow', 'was', 'a', 'technical', 'advisor', 'to', 'the', 'prime', 'minister', 'of', 'chad', 'for', 'microfinance', 'and', 'sustainable', 'development', 'from', 'february', '2013', 'to', 'october', '2013', 'sow', 'was', 'a', 'technical', 'advisor', 'for', 'economic', 'and', 'budgetary', 'affairs', 'at', 'the', 'presidency', 'of', 'the', 'republic', 'from', 'october', '2013', 'to', 'april', '2014', 'sow', 'held', 'the', 'post', 'of', 'minister', 'of', 'microcredits', 'for', 'the', 'promotion', 'of', 'women', 'and', 'youth', 'from', 'april', '2014', 'to', 'august', '2015', 'sow', 'was', 'the', 'secretary', 'of', 'state', 'for', 'finance', 'and', 'budget', 'in', 'charge', 'of', 'microfinance', 'from', 'november', '2015', 'to', 'august', '2016', 'sow', 'was', 'the', 'secretary', 'general', 'of', 'the', 'court', 'of', 'auditors', 'from', 'august', '2016', 'to', 'february', '2017', 'sow', 'was', 'the', 'secretary', 'of', 'state', 'for', 'infrastructure', 'and', 'opening', 'up', 'from', 'february', '5', '2017', 'to', 'november', '21', '2017', 'sow', 'was', 'the', 'secretary', 'of', 'state', 'for', 'finance', 'and', 'budget', 'as', 'of', 'june', '2018', 'sow', 'was', 'the', 'chief', 'of', 'staff', 'to', 'the', 'president', 'of', 'chad']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Read in the file and create a long sequence of tokens for\n",
    "# all tokens in the file\n",
    "file_name = \"wiki-bios.DEBUG.txt\"\n",
    "all_tokens = []\n",
    "punct_re = re.compile(r\"[^\\w\\s]\")\n",
    "print('Reading data and tokenizing\\n---------------------------------------')\n",
    "with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    #print(lines[0])\n",
    "    for line in lines:\n",
    "        new_line = punct_re.sub('', line).lower().split()\n",
    "        for token in new_line:\n",
    "            all_tokens.append(token)\n",
    "        #print(new_line)\n",
    "        break\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89f7b996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting token frequencies\n",
      "-------------------------------------------------------------------\n",
      "{'career': 1, 'from': 7, 'june': 2, '2009': 1, 'to': 9, 'february': 4, '2013': 4, 'sow': 8, 'was': 7, 'a': 2, 'technical': 2, 'advisor': 2, 'the': 12, 'prime': 1, 'minister': 2, 'of': 14, 'chad': 2, 'for': 6, 'microfinance': 2, 'and': 6, 'sustainable': 1, 'development': 1, 'october': 2, 'economic': 1, 'budgetary': 1, 'affairs': 1, 'at': 1, 'presidency': 1, 'republic': 1, 'april': 2, '2014': 2, 'held': 1, 'post': 1, 'microcredits': 1, 'promotion': 1, 'women': 1, 'youth': 1, 'august': 3, '2015': 2, 'secretary': 4, 'state': 3, 'finance': 2, 'budget': 2, 'in': 1, 'charge': 1, 'november': 2, '2016': 2, 'general': 1, 'court': 1, 'auditors': 1, '2017': 3, 'infrastructure': 1, 'opening': 1, 'up': 1, '5': 1, '21': 1, 'as': 1, '2018': 1, 'chief': 1, 'staff': 1, 'president': 1}\n",
      "Counter({'of': 14, 'the': 12, 'to': 9, 'sow': 8, 'from': 7, 'was': 7, 'for': 6, 'and': 6, 'february': 4, '2013': 4, 'secretary': 4, 'august': 3, 'state': 3, '2017': 3, 'june': 2, 'a': 2, 'technical': 2, 'advisor': 2, 'minister': 2, 'chad': 2, 'microfinance': 2, 'october': 2, 'april': 2, '2014': 2, '2015': 2, 'finance': 2, 'budget': 2, 'november': 2, '2016': 2, 'career': 1, '2009': 1, 'prime': 1, 'sustainable': 1, 'development': 1, 'economic': 1, 'budgetary': 1, 'affairs': 1, 'at': 1, 'presidency': 1, 'republic': 1, 'held': 1, 'post': 1, 'microcredits': 1, 'promotion': 1, 'women': 1, 'youth': 1, 'in': 1, 'charge': 1, 'general': 1, 'court': 1, 'auditors': 1, 'infrastructure': 1, 'opening': 1, 'up': 1, '5': 1, '21': 1, 'as': 1, '2018': 1, 'chief': 1, 'staff': 1, 'president': 1})\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Count how many tokens we have of each type\n",
    "print('Counting token frequencies\\n-------------------------------------------------------------------')\n",
    "min_word_freq = 5\n",
    "\n",
    "#Counter object might do the same thing\n",
    "vocabulary = {}\n",
    "for token in all_tokens:\n",
    "    if token not in vocabulary.keys():\n",
    "        vocabulary[token] = 1\n",
    "    elif token in vocabulary.keys():\n",
    "        vocabulary[token] += 1\n",
    "print(vocabulary)\n",
    "\n",
    "token_freq = Counter(all_tokens)\n",
    "print(token_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cafc054c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<UNK>', 'from', '<UNK>', '<UNK>', 'to', '<UNK>', '<UNK>', 'sow', 'was', '<UNK>', '<UNK>', '<UNK>', 'to', 'the', '<UNK>', '<UNK>', 'of', '<UNK>', 'for', '<UNK>', 'and', '<UNK>', '<UNK>', 'from', '<UNK>', '<UNK>', 'to', '<UNK>', '<UNK>', 'sow', 'was', '<UNK>', '<UNK>', '<UNK>', 'for', '<UNK>', 'and', '<UNK>', '<UNK>', '<UNK>', 'the', '<UNK>', 'of', 'the', '<UNK>', 'from', '<UNK>', '<UNK>', 'to', '<UNK>', '<UNK>', 'sow', '<UNK>', 'the', '<UNK>', 'of', '<UNK>', 'of', '<UNK>', 'for', 'the', '<UNK>', 'of', '<UNK>', 'and', '<UNK>', 'from', '<UNK>', '<UNK>', 'to', '<UNK>', '<UNK>', 'sow', 'was', 'the', '<UNK>', 'of', '<UNK>', 'for', '<UNK>', 'and', '<UNK>', '<UNK>', '<UNK>', 'of', '<UNK>', 'from', '<UNK>', '<UNK>', 'to', '<UNK>', '<UNK>', 'sow', 'was', 'the', '<UNK>', '<UNK>', 'of', 'the', '<UNK>', 'of', '<UNK>', 'from', '<UNK>', '<UNK>', 'to', '<UNK>', '<UNK>', 'sow', 'was', 'the', '<UNK>', 'of', '<UNK>', 'for', '<UNK>', 'and', '<UNK>', '<UNK>', 'from', '<UNK>', '<UNK>', '<UNK>', 'to', '<UNK>', '<UNK>', '<UNK>', 'sow', 'was', 'the', '<UNK>', 'of', '<UNK>', 'for', '<UNK>', 'and', '<UNK>', '<UNK>', 'of', '<UNK>', '<UNK>', 'sow', 'was', 'the', '<UNK>', 'of', '<UNK>', 'to', 'the', '<UNK>', 'of', '<UNK>']\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Replace all tokens below the specified frequency with an <UNK> token. \n",
    "# actually replace the words with unk token in the document? \n",
    "\n",
    "min_token_freq = 5\n",
    "updated_tokens = []\n",
    "for token in all_tokens:\n",
    "    total_token_freq = token_freq[token]\n",
    "    if total_token_freq < min_token_freq:\n",
    "        new_token = '<UNK>'\n",
    "        updated_tokens.append(new_token)\n",
    "    else:\n",
    "        updated_tokens.append(token)\n",
    "print(updated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9661291d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "# Step 4: update self.word_counts to be the number of times each word\n",
    "# occurs (including <UNK>)\n",
    "# corpus object attribute \"word_counts\"\n",
    "\n",
    "# just assign the vocabulary dict as the word_counts attribute\n",
    "print(len(vocabulary))\n",
    "print(len(token_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5227ef15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['career', 'from', 'june', '2009', 'to', 'february', '2013', 'sow', 'was', 'a', 'technical', 'advisor', 'to', 'the', 'prime', 'minister', 'of', 'chad', 'for', 'microfinance', 'and', 'sustainable', 'development', 'from', 'february', '2013', 'to', 'october', '2013', 'sow', 'was', 'a', 'technical', 'advisor', 'for', 'economic', 'and', 'budgetary', 'affairs', 'at', 'the', 'presidency', 'of', 'the', 'republic', 'from', 'october', '2013', 'to', 'april', '2014', 'sow', 'held', 'the', 'post', 'of', 'minister', 'of', 'microcredits', 'for', 'the', 'promotion', 'of', 'women', 'and', 'youth', 'from', 'april', '2014', 'to', 'august', '2015', 'sow', 'was', 'the', 'secretary', 'of', 'state', 'for', 'finance', 'and', 'budget', 'in', 'charge', 'of', 'microfinance', 'from', 'november', '2015', 'to', 'august', '2016', 'sow', 'was', 'the', 'secretary', 'general', 'of', 'the', 'court', 'of', 'auditors', 'from', 'august', '2016', 'to', 'february', '2017', 'sow', 'was', 'the', 'secretary', 'of', 'state', 'for', 'infrastructure', 'and', 'opening', 'up', 'from', 'february', '5', '2017', 'to', 'november', '21', '2017', 'sow', 'was', 'the', 'secretary', 'of', 'state', 'for', 'finance', 'and', 'budget', 'as', 'of', 'june', '2018', 'sow', 'was', 'the', 'chief', 'of', 'staff', 'to', 'the', 'president', 'of', 'chad']\n",
      "{0: 'career', 1: 'from', 2: 'june', 3: '2009', 4: 'to', 5: 'february', 6: '2013', 7: 'sow', 8: 'was', 9: 'a', 10: 'technical', 11: 'advisor', 12: 'the', 13: 'prime', 14: 'minister', 15: 'of', 16: 'chad', 17: 'for', 18: 'microfinance', 19: 'and', 20: 'sustainable', 21: 'development', 22: 'october', 23: 'economic', 24: 'budgetary', 25: 'affairs', 26: 'at', 27: 'presidency', 28: 'republic', 29: 'april', 30: '2014', 31: 'held', 32: 'post', 33: 'microcredits', 34: 'promotion', 35: 'women', 36: 'youth', 37: 'august', 38: '2015', 39: 'secretary', 40: 'state', 41: 'finance', 42: 'budget', 43: 'in', 44: 'charge', 45: 'november', 46: '2016', 47: 'general', 48: 'court', 49: 'auditors', 50: '2017', 51: 'infrastructure', 52: 'opening', 53: 'up', 54: '5', 55: '21', 56: 'as', 57: '2018', 58: 'chief', 59: 'staff', 60: 'president'}\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Create the mappings from word to unique integer ID and the\n",
    "# reverse mapping.\n",
    "\n",
    "int_id = 0\n",
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "for token in vocabulary.keys():\n",
    "    word_to_id[token] = int_id\n",
    "    id_to_word[int_id] = token\n",
    "    int_id += 1\n",
    "\n",
    "print(all_tokens)\n",
    "print(id_to_word)\n",
    "print(word_to_id['chad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b254c7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip step 6 for now - probabilistic subsampling - first work on the generate_negative_sampling_table function\n",
    "# Step 6: Compute the probability of keeping any particular *token* of a\n",
    "# word in the training sequence, which we'll use to subsample. This subsampling\n",
    "# avoids having the training data be filled with many overly common words\n",
    "# as positive examples in the context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5846f25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['career', 'from', 'june', '2009', 'to', 'february', '2013', 'sow', 'was', 'a', 'technical', 'advisor', 'to', 'the', 'prime', 'minister', 'of', 'chad', 'for', 'microfinance', 'and', 'sustainable', 'development', 'from', 'february', '2013', 'to', 'october', '2013', 'sow', 'was', 'a', 'technical', 'advisor', 'for', 'economic', 'and', 'budgetary', 'affairs', 'at', 'the', 'presidency', 'of', 'the', 'republic', 'from', 'october', '2013', 'to', 'april', '2014', 'sow', 'held', 'the', 'post', 'of', 'minister', 'of', 'microcredits', 'for', 'the', 'promotion', 'of', 'women', 'and', 'youth', 'from', 'april', '2014', 'to', 'august', '2015', 'sow', 'was', 'the', 'secretary', 'of', 'state', 'for', 'finance', 'and', 'budget', 'in', 'charge', 'of', 'microfinance', 'from', 'november', '2015', 'to', 'august', '2016', 'sow', 'was', 'the', 'secretary', 'general', 'of', 'the', 'court', 'of', 'auditors', 'from', 'august', '2016', 'to', 'february', '2017', 'sow', 'was', 'the', 'secretary', 'of', 'state', 'for', 'infrastructure', 'and', 'opening', 'up', 'from', 'february', '5', '2017', 'to', 'november', '21', '2017', 'sow', 'was', 'the', 'secretary', 'of', 'state', 'for', 'finance', 'and', 'budget', 'as', 'of', 'june', '2018', 'sow', 'was', 'the', 'chief', 'of', 'staff', 'to', 'the', 'president', 'of', 'chad']\n",
      "-------------------------------------------------\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 4, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 1, 5, 6, 4, 22, 6, 7, 8, 9, 10, 11, 17, 23, 19, 24, 25, 26, 12, 27, 15, 12, 28, 1, 22, 6, 4, 29, 30, 7, 31, 12, 32, 15, 14, 15, 33, 17, 12, 34, 15, 35, 19, 36, 1, 29, 30, 4, 37, 38, 7, 8, 12, 39, 15, 40, 17, 41, 19, 42, 43, 44, 15, 18, 1, 45, 38, 4, 37, 46, 7, 8, 12, 39, 47, 15, 12, 48, 15, 49, 1, 37, 46, 4, 5, 50, 7, 8, 12, 39, 15, 40, 17, 51, 19, 52, 53, 1, 5, 54, 50, 4, 45, 55, 50, 7, 8, 12, 39, 15, 40, 17, 41, 19, 42, 56, 15, 2, 57, 7, 8, 12, 58, 15, 59, 4, 12, 60, 15, 16]\n",
      "career\n",
      "from\n",
      "june\n",
      "2009\n",
      "to\n",
      "february\n",
      "2013\n",
      "sow\n",
      "was\n",
      "a\n",
      "technical\n",
      "advisor\n",
      "to\n",
      "the\n",
      "prime\n",
      "minister\n",
      "of\n",
      "chad\n",
      "for\n",
      "microfinance\n",
      "and\n",
      "sustainable\n",
      "development\n",
      "from\n",
      "february\n",
      "2013\n",
      "to\n",
      "october\n",
      "2013\n",
      "sow\n",
      "was\n",
      "a\n",
      "technical\n",
      "advisor\n",
      "for\n",
      "economic\n",
      "and\n",
      "budgetary\n",
      "affairs\n",
      "at\n",
      "the\n",
      "presidency\n",
      "of\n",
      "the\n",
      "republic\n",
      "from\n",
      "october\n",
      "2013\n",
      "to\n",
      "april\n",
      "2014\n",
      "sow\n",
      "held\n",
      "the\n",
      "post\n",
      "of\n",
      "minister\n",
      "of\n",
      "microcredits\n",
      "for\n",
      "the\n",
      "promotion\n",
      "of\n",
      "women\n",
      "and\n",
      "youth\n",
      "from\n",
      "april\n",
      "2014\n",
      "to\n",
      "august\n",
      "2015\n",
      "sow\n",
      "was\n",
      "the\n",
      "secretary\n",
      "of\n",
      "state\n",
      "for\n",
      "finance\n",
      "and\n",
      "budget\n",
      "in\n",
      "charge\n",
      "of\n",
      "microfinance\n",
      "from\n",
      "november\n",
      "2015\n",
      "to\n",
      "august\n",
      "2016\n",
      "sow\n",
      "was\n",
      "the\n",
      "secretary\n",
      "general\n",
      "of\n",
      "the\n",
      "court\n",
      "of\n",
      "auditors\n",
      "from\n",
      "august\n",
      "2016\n",
      "to\n",
      "february\n",
      "2017\n",
      "sow\n",
      "was\n",
      "the\n",
      "secretary\n",
      "of\n",
      "state\n",
      "for\n",
      "infrastructure\n",
      "and\n",
      "opening\n",
      "up\n",
      "from\n",
      "february\n",
      "5\n",
      "2017\n",
      "to\n",
      "november\n",
      "21\n",
      "2017\n",
      "sow\n",
      "was\n",
      "the\n",
      "secretary\n",
      "of\n",
      "state\n",
      "for\n",
      "finance\n",
      "and\n",
      "budget\n",
      "as\n",
      "of\n",
      "june\n",
      "2018\n",
      "sow\n",
      "was\n",
      "the\n",
      "chief\n",
      "of\n",
      "staff\n",
      "to\n",
      "the\n",
      "president\n",
      "of\n",
      "chad\n"
     ]
    }
   ],
   "source": [
    "# step 7a - generate a list for the sequence of the text using the token mappings\n",
    "full_token_sequence_as_ids = []\n",
    "for token in all_tokens:\n",
    "    word_id = word_to_id[token]\n",
    "    full_token_sequence_as_ids.append(word_id)\n",
    "print(all_tokens)\n",
    "print('-------------------------------------------------')\n",
    "print(full_token_sequence_as_ids)\n",
    "for token_id in full_token_sequence_as_ids:\n",
    "    token = id_to_word[token_id]\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de76eed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 7b\n",
    "# Step 7: process the list of tokens (after min-freq filtering) to fill\n",
    "# a new list self.full_token_sequence_as_ids where \n",
    "#\n",
    "# (1) we probabilistically choose whether to keep each *token* based on the\n",
    "# subsampling probabilities (note that this does not mean we drop\n",
    "# an entire word!) and \n",
    "#\n",
    "# (2) all tokens are convered to their unique ids for faster training.\n",
    "#\n",
    "# NOTE: You can skip the subsampling part and just do step 2 to get\n",
    "# your model up and running.\n",
    "\n",
    "# NOTE 2: You will perform token-based subsampling based on the probabilities in\n",
    "# word_to_sample_prob. When subsampling, you are modifying the sequence itself \n",
    "# (like deleting an item in a list). This action effectively makes the context\n",
    "# window  larger for some target words by removing context words that are common\n",
    "# from a particular context before the training occurs (which then would now include\n",
    "# other words that were previously just outside the window)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ea5f1e",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Modify function generate negative sampling table to create the negative sampling table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2173a454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 7, 2, 1, 9, 4, 4, 8, 7, 2, 2, 2, 12, 1, 2, 14, 2, 6, 2, 6, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 3, 2, 4, 3, 2, 2, 1, 1, 2, 2, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1.         4.30351707 1.68179283 1.         5.19615242 2.82842712\n",
      " 2.82842712 4.75682846 4.30351707 1.68179283 1.68179283 1.68179283\n",
      " 6.44741959 1.         1.68179283 7.23762416 1.68179283 3.83365863\n",
      " 1.68179283 3.83365863 1.         1.         1.68179283 1.\n",
      " 1.         1.         1.         1.         1.         1.68179283\n",
      " 1.68179283 1.         1.         1.         1.         1.\n",
      " 1.         2.27950706 1.68179283 2.82842712 2.27950706 1.68179283\n",
      " 1.68179283 1.         1.         1.68179283 1.68179283 1.\n",
      " 1.         1.         2.27950706 1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.        ]\n",
      "112.46307102404646\n",
      "[0.00889181 0.03826605 0.01495418 0.00889181 0.04620319 0.02514983\n",
      " 0.02514983 0.0422968  0.03826605 0.01495418 0.01495418 0.01495418\n",
      " 0.05732922 0.00889181 0.01495418 0.06435556 0.01495418 0.03408816\n",
      " 0.01495418 0.03408816 0.00889181 0.00889181 0.01495418 0.00889181\n",
      " 0.00889181 0.00889181 0.00889181 0.00889181 0.00889181 0.01495418\n",
      " 0.01495418 0.00889181 0.00889181 0.00889181 0.00889181 0.00889181\n",
      " 0.00889181 0.02026894 0.01495418 0.02514983 0.02026894 0.01495418\n",
      " 0.01495418 0.00889181 0.00889181 0.01495418 0.01495418 0.00889181\n",
      " 0.00889181 0.00889181 0.02026894 0.00889181 0.00889181 0.00889181\n",
      " 0.00889181 0.00889181 0.00889181 0.00889181 0.00889181 0.00889181\n",
      " 0.00889181]\n",
      "[ 8891.80769202 38266.04619163 14954.17842669  8891.80769202\n",
      " 46203.18808114 25149.83006414 25149.83006414 42296.80389035\n",
      " 38266.04619163 14954.17842669 14954.17842669 14954.17842669\n",
      " 57329.21511242  8891.80769202 14954.17842669 64355.56213695\n",
      " 14954.17842669 34088.15525461 14954.17842669 34088.15525461\n",
      "  8891.80769202  8891.80769202 14954.17842669  8891.80769202\n",
      "  8891.80769202  8891.80769202  8891.80769202  8891.80769202\n",
      "  8891.80769202 14954.17842669 14954.17842669  8891.80769202\n",
      "  8891.80769202  8891.80769202  8891.80769202  8891.80769202\n",
      "  8891.80769202 20268.93838305 14954.17842669 25149.83006414\n",
      " 20268.93838305 14954.17842669 14954.17842669  8891.80769202\n",
      "  8891.80769202 14954.17842669 14954.17842669  8891.80769202\n",
      "  8891.80769202  8891.80769202 20268.93838305  8891.80769202\n",
      "  8891.80769202  8891.80769202  8891.80769202  8891.80769202\n",
      "  8891.80769202  8891.80769202  8891.80769202  8891.80769202\n",
      "  8891.80769202]\n",
      "chad\n",
      "14954.178426692919\n"
     ]
    }
   ],
   "source": [
    "exp_power = .75\n",
    "table_size= 1e6\n",
    "print(list(token_freq.values()))\n",
    "word_weight = np.power(list(token_freq.values()),exp_power)\n",
    "print(word_weight)\n",
    "sum_of_weight = np.sum(word_weight)\n",
    "print(sum_of_weight)\n",
    "word_probability = word_weight/sum_of_weight\n",
    "print(word_probability)\n",
    "word_table = word_probability * table_size\n",
    "print(word_table)\n",
    "\n",
    "print(id_to_word[16])\n",
    "print(word_table[16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3395b61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sampling table\n",
      "2\n",
      "1.681792830507429\n",
      "0.014954178426692919\n",
      "61\n",
      "{0: 8892.0, 1: 38266.0, 2: 14954.0, 3: 8892.0, 4: 46203.0, 5: 25150.0, 6: 25150.0, 7: 42297.0, 8: 38266.0, 9: 14954.0, 10: 14954.0, 11: 14954.0, 12: 57329.0, 13: 8892.0, 14: 14954.0, 15: 64356.0, 16: 14954.0, 17: 34088.0, 18: 14954.0, 19: 34088.0, 20: 8892.0, 21: 8892.0, 22: 14954.0, 23: 8892.0, 24: 8892.0, 25: 8892.0, 26: 8892.0, 27: 8892.0, 28: 8892.0, 29: 14954.0, 30: 14954.0, 31: 8892.0, 32: 8892.0, 33: 8892.0, 34: 8892.0, 35: 8892.0, 36: 8892.0, 37: 20269.0, 38: 14954.0, 39: 25150.0, 40: 20269.0, 41: 14954.0, 42: 14954.0, 43: 8892.0, 44: 8892.0, 45: 14954.0, 46: 14954.0, 47: 8892.0, 48: 8892.0, 49: 8892.0, 50: 20269.0, 51: 8892.0, 52: 8892.0, 53: 8892.0, 54: 8892.0, 55: 8892.0, 56: 8892.0, 57: 8892.0, 58: 8892.0, 59: 8892.0, 60: 8892.0}\n",
      "[0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "exp_power = 0.75\n",
    "table_size=1e6\n",
    "# Step 1: Figure out how many instances of each word need to go into the\n",
    "# negative sampling table. \n",
    "#\n",
    "# HINT: np.power and np.fill might be useful here        \n",
    "print(\"Generating sampling table\")\n",
    "id_freq = {}\n",
    "for token, count in token_freq.items():\n",
    "    token_id = word_to_id[token]\n",
    "    id_freq[token_id] = count\n",
    "print(id_freq[16])\n",
    "\n",
    "id_weights = np.power(list(token_freq.values()),exp_power)\n",
    "sum_of_weight = np.sum(id_weights)\n",
    "id_probas = id_weights/sum_of_weight\n",
    "print(id_weights[16])\n",
    "print(id_probas[16])\n",
    "print(len(id_probas))\n",
    "\n",
    "# Step 2: Create the table to the correct size. You'll want this to be a\n",
    "# numpy array of type int\n",
    "num_ids_in_tbl = dict(zip(id_freq.keys(), np.around(word_probability * table_size)))\n",
    "print(num_ids_in_tbl)\n",
    "\n",
    "# Step 3: Fill the table so that each word has a number of IDs\n",
    "# proportionate to its probability of being sampled.\n",
    "#\n",
    "# Example: if we have 3 words \"a\" \"b\" and \"c\" with probabilites 0.5,\n",
    "# 0.33, 0.16 and a table size of 6 then our table would look like this\n",
    "# (before converting the words to IDs):\n",
    "#\n",
    "# [ \"a\", \"a\", \"a\", \"b\", \"b\", \"c\" ]\n",
    "#\n",
    "negsamp_tbl = []\n",
    "for tid, inst in num_ids_in_tbl.items():\n",
    "# fill negsamp_tbl with word_id x number of times (where x=num_ids[word_id])\n",
    "    tbl_fill = [tid] * int(inst)\n",
    "    negsamp_tbl += tbl_fill\n",
    "print(negsamp_tbl[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed81942",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "Generate the list of training instances according to the specifications in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "811bec5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 17]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Randomly samples the specified number of negative samples from the lookup\n",
    "table and returns this list of IDs as a numpy array. As a performance\n",
    "improvement, avoid sampling a negative example that has the same ID as\n",
    "the current positive context word.\n",
    "'''\n",
    "num_samples = 2\n",
    "num_context_words = 2\n",
    "cur_context_word_id = 16 #current positive context word\n",
    "results = []\n",
    "\n",
    "# Create a list and sample from the negative_sampling_table to\n",
    "# grow the list to num_samples, avoiding adding a negative example that\n",
    "# has the same ID as the current context_word\n",
    "# rand_sample = np.random.randint(low=0, high=len(negsamp_tbl), size=num_samples).tolist()\n",
    "# print(rand_sample)\n",
    "# #neg_samples = [negsamp_tbl[rand_sample[0]], negsamp_tbl[rand_sample[1]]]\n",
    "# neg_samples = [16, 48]\n",
    "# print(neg_samples)\n",
    "rand_sample = np.random.randint(low=0, high=len(negsamp_tbl), size=num_samples).tolist()\n",
    "neg_samples = [negsamp_tbl[rand_sample[0]], negsamp_tbl[rand_sample[1]]]\n",
    "print(neg_samples)\n",
    "while neg_samples[0] == cur_context_word_id or neg_samples[1] == cur_context_word_id:\n",
    "    rand_sample = np.random.randint(low=0, high=len(negsamp_tbl), size=num_samples).tolist()\n",
    "    neg_samples = [negsamp_tbl[rand_sample[0]], negsamp_tbl[rand_sample[1]]]\n",
    "    print(neg_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "06cc13ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "while len(results) < num_samples:\n",
    "    rand_sample = np.random.randint(low=0, high=len(negsamp_tbl))\n",
    "    neg_sample = negsamp_tbl[rand_sample]\n",
    "    print(neg_sample)\n",
    "    if neg_sample != cur_context_word_id:\n",
    "        results.append(neg_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c82dd9",
   "metadata": {},
   "source": [
    "## Test Corpus Class Definition\n",
    "\n",
    "Test after implementing problems 1-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "massive-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        \n",
    "        # These state variables become populated with function calls\n",
    "        #\n",
    "        # 1. load_data()\n",
    "        # 2. generate_negative_sampling_table()\n",
    "        #\n",
    "        # See those functions for how the various values get filled in\n",
    "\n",
    "        self.word_to_index = {} # word to unique-id\n",
    "        self.index_to_word = {} # unique-id to word\n",
    "\n",
    "        # How many times each word occurs in our data after filtering\n",
    "        self.word_counts = Counter()\n",
    "\n",
    "        # A utility data structure that lets us quickly sample \"negative\"\n",
    "        # instances in a context. This table contains unique-ids\n",
    "        self.negative_sampling_table = []\n",
    "        \n",
    "        # The dataset we'll use for training, as a sequence of unqiue word\n",
    "        # ids. This is the sequence across all documents after tokens have been\n",
    "        # randomly subsampled by the word2vec preprocessing step\n",
    "        self.full_token_sequence_as_ids = []\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        '''\n",
    "        Tokenize the document and returns a list of the tokens\n",
    "        '''\n",
    "        return self.tokenizer.tokenize(text)        \n",
    "\n",
    "    def load_data(self, file_name, min_token_freq):\n",
    "        '''\n",
    "        Reads the data from the specified file as long long sequence of text\n",
    "        (ignoring line breaks) and populates the data structures of this\n",
    "        word2vec object.\n",
    "        '''\n",
    "\n",
    "        # Step 1: Read in the file and create a long sequence of tokens for\n",
    "        # all tokens in the file\n",
    "        all_tokens = []\n",
    "        print('Reading data and tokenizing')\n",
    "        punct_re = re.compile(r\"[^\\w\\s]\")\n",
    "        with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                new_line = punct_re.sub('', line).lower().split()\n",
    "                for token in new_line:\n",
    "                    all_tokens.append(token)\n",
    "                break\n",
    "    \n",
    "        # Step 2: Count how many tokens we have of each type\n",
    "        print('Counting token frequencies')\n",
    "        token_freq = Counter(all_tokens)\n",
    "\n",
    "        # Step 3: Replace all tokens below the specified frequency with an <UNK>\n",
    "        # token. \n",
    "        #\n",
    "        # NOTE: You can do this step later if needed\n",
    "        print(\"Performing minimum thresholding\")\n",
    "        updated_tokens = []\n",
    "        for token in all_tokens:\n",
    "            total_token_freq = token_freq[token]\n",
    "            if total_token_freq < min_token_freq:\n",
    "                new_token = '<UNK>'\n",
    "                updated_tokens.append(new_token)\n",
    "            else:\n",
    "                updated_tokens.append(token)\n",
    "\n",
    "        # Step 4: update self.word_counts to be the number of times each word\n",
    "        # occurs (including <UNK>)\n",
    "        self.word_counts = Counter(updated_tokens)\n",
    "        \n",
    "        # Step 5: Create the mappings from word to unique integer ID and the\n",
    "        # reverse mapping.\n",
    "        int_id = 0\n",
    "        for token in self.word_counts.keys():\n",
    "            self.word_to_index[token] = int_id\n",
    "            self.index_to_word[int_id] = token\n",
    "            int_id += 1\n",
    "        \n",
    "        # Step 6: Compute the probability of keeping any particular *token* of a\n",
    "        # word in the training sequence, which we'll use to subsample. This subsampling\n",
    "        # avoids having the training data be filled with many overly common words\n",
    "        # as positive examples in the context\n",
    "                        \n",
    "        # Step 7: process the list of tokens (after min-freq filtering) to fill\n",
    "        # a new list self.full_token_sequence_as_ids where \n",
    "        #\n",
    "        # (1) we probabilistically choose whether to keep each *token* based on the\n",
    "        # subsampling probabilities (note that this does not mean we drop\n",
    "        # an entire word!) and \n",
    "        #\n",
    "        # (2) all tokens are convered to their unique ids for faster training.\n",
    "        #\n",
    "        # NOTE: You can skip the subsampling part and just do step 2 to get\n",
    "        # your model up and running.\n",
    "            \n",
    "        # NOTE 2: You will perform token-based subsampling based on the probabilities in\n",
    "        # word_to_sample_prob. When subsampling, you are modifying the sequence itself \n",
    "        # (like deleting an item in a list). This action effectively makes the context\n",
    "        # window  larger for some target words by removing context words that are common\n",
    "        # from a particular context before the training occurs (which then would now include\n",
    "        # other words that were previously just outside the window).\n",
    "        for token in all_tokens:\n",
    "            word_index = self.word_to_index[token]\n",
    "            self.full_token_sequence_as_ids.append(word_index)\n",
    "\n",
    "\n",
    "        # Helpful print statement to verify what you've loaded\n",
    "        print('Loaded all data from %s; saw %d tokens (%d unique)' \\\n",
    "              % (file_name, len(self.full_token_sequence_as_ids),\n",
    "                 len(self.word_to_index)))\n",
    "        \n",
    "    def generate_negative_sampling_table(self, exp_power=0.75, table_size=1e6):\n",
    "        '''\n",
    "        Generates a big list data structure that we can quickly randomly index into\n",
    "        in order to select a negative training example (i.e., a word that was\n",
    "        *not* present in the context). \n",
    "        '''       \n",
    "        \n",
    "        # Step 1: Figure out how many instances of each word need to go into the\n",
    "        # negative sampling table. \n",
    "        #\n",
    "        # HINT: np.power and np.fill might be useful here        \n",
    "        print(\"Generating sampling table\")\n",
    "        id_freq = {}\n",
    "        for token, count in token_freq.items():\n",
    "            token_id = word_to_id[token]\n",
    "            id_freq[token_id] = count\n",
    "\n",
    "        id_weights = np.power(list(token_freq.values()),exp_power)\n",
    "        sum_of_weight = np.sum(id_weights)\n",
    "        id_probas = id_weights/sum_of_weight\n",
    "\n",
    "        # Step 2: Create the table to the correct size. You'll want this to be a\n",
    "        # numpy array of type int\n",
    "        num_ids_in_tbl = dict(zip(id_freq.keys(), np.around(word_probability * table_size)))\n",
    "\n",
    "        # Step 3: Fill the table so that each word has a number of IDs\n",
    "        # proportionate to its probability of being sampled.\n",
    "        #\n",
    "        # Example: if we have 3 words \"a\" \"b\" and \"c\" with probabilites 0.5,\n",
    "        # 0.33, 0.16 and a table size of 6 then our table would look like this\n",
    "        # (before converting the words to IDs):\n",
    "        #\n",
    "        # [ \"a\", \"a\", \"a\", \"b\", \"b\", \"c\" ]\n",
    "        #\n",
    "        negsamp_tbl = []\n",
    "        for tid, inst in num_ids_in_tbl.items():\n",
    "            tbl_fill = [tid] * int(inst)\n",
    "            negsamp_tbl += tbl_fill\n",
    "        return negsamp_tbl\n",
    "\n",
    "\n",
    "    def generate_negative_samples(self, cur_context_word_id, num_samples):\n",
    "        '''\n",
    "        Randomly samples the specified number of negative samples from the lookup\n",
    "        table and returns this list of IDs as a numpy array. As a performance\n",
    "        improvement, avoid sampling a negative example that has the same ID as\n",
    "        the current positive context word.\n",
    "        '''\n",
    "\n",
    "        results = []\n",
    "\n",
    "        # Create a list and sample from the negative_sampling_table to\n",
    "        # grow the list to num_samples, avoiding adding a negative example that\n",
    "        # has the same ID as the current context_word\n",
    "#         rand_sample = np.random.randint(low=0, high=len(negsamp_tbl), size=num_samples).tolist()\n",
    "#         neg_samples = [negsamp_tbl[rand_sample[0]], negsamp_tbl[rand_sample[1]]]\n",
    "#         while neg_samples[0] == cur_context_word_id or neg_samples[1] == cur_context_word_id:\n",
    "#             rand_sample = np.random.randint(low=0, high=len(negsamp_tbl), size=num_samples).tolist()\n",
    "#             neg_samples = [negsamp_tbl[rand_sample[0]], negsamp_tbl[rand_sample[1]]]\n",
    "        \n",
    "        while len(results) < num_samples:\n",
    "            rand_sample = np.random.randint(low=0, high=len(negsamp_tbl))\n",
    "            neg_sample = negsamp_tbl[rand_sample]\n",
    "            if neg_sample != cur_context_word_id:\n",
    "                results.append(neg_sample)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "revised-salmon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data and tokenizing\n",
      "Counting token frequencies\n",
      "Performing minimum thresholding\n",
      "Loaded all data from wiki-bios.DEBUG.txt; saw 152 tokens (61 unique)\n"
     ]
    }
   ],
   "source": [
    "test_file_name = \"wiki-bios.DEBUG.txt\"\n",
    "test_corpus = Corpus()\n",
    "test_corpus.load_data(test_file_name,min_token_freq=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ec9a35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
